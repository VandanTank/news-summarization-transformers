{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c30e757",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Define the models we want to compare\n",
    "model_checkpoints = {\n",
    "    \"BART\": \"facebook/bart-base\",\n",
    "    \"T5\": \"t5-small\",\n",
    "    \"PEGASUS\": \"google/pegasus-xsum\"\n",
    "}\n",
    "\n",
    "# Dictionaries to store our loaded models and tokenizers\n",
    "tokenizers = {}\n",
    "models = {}\n",
    "\n",
    "print(\"--- Loading Models and Tokenizers ---\")\n",
    "\n",
    "for model_name, checkpoint in model_checkpoints.items():\n",
    "    print(f\"Loading {model_name} from checkpoint: {checkpoint}\")\n",
    "\n",
    "    # Load the tokenizer\n",
    "    tokenizers[model_name] = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "    # Load the pre-trained model\n",
    "    models[model_name] = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "\n",
    "print(\"\\n--- All models and tokenizers loaded successfully! ---\")\n",
    "print(\"\\nTokenizers loaded:\", tokenizers.keys())\n",
    "print(\"Models loaded:\", models.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5001f5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# --- Re-load our data from Step 2 ---\n",
    "# Define the direct URLs to the script-free Parquet files\n",
    "data_files = {\n",
    "    \"train\": \"https://huggingface.co/datasets/EdinburghNLP/xsum/resolve/refs%2Fconvert%2Fparquet/default/train/0000.parquet\",\n",
    "    \"validation\": \"https://huggingface.co/datasets/EdinburghNLP/xsum/resolve/refs%2Fconvert%2Fparquet/default/validation/0000.parquet\",\n",
    "    \"test\": \"https://huggingface.co/datasets/EdinburghNLP/xsum/resolve/refs%2Fconvert%2Fparquet/default/test/0000.parquet\"\n",
    "}\n",
    "\n",
    "# Load the dataset by pointing directly to the Parquet files\n",
    "print(\"Loading XSum dataset from direct Parquet URLs...\")\n",
    "dataset = load_dataset(\"parquet\", data_files=data_files)\n",
    "\n",
    "# --- Re-sample our data from Step 2 ---\n",
    "train_sample = dataset['train'].shuffle(seed=42).select(range(5000))\n",
    "validation_sample = dataset['validation'].shuffle(seed=42).select(range(1000))\n",
    "test_sample = dataset['test'].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "# Create the final sampled DatasetDict\n",
    "sampled_dataset = DatasetDict({\n",
    "    'train': train_sample,\n",
    "    'validation': validation_sample,\n",
    "    'test': test_sample\n",
    "})\n",
    "\n",
    "print(\"\\n--- Final Sampled DatasetDict ---\")\n",
    "print(sampled_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8eeef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set max lengths for our input and output\n",
    "# We truncate the article to 512 tokens and the summary to 128\n",
    "max_input_length = 512\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess_function(examples, tokenizer, model_name):\n",
    "    # --- Prepare Input ---\n",
    "    # T5 models require a \"summarize: \" prefix\n",
    "    if model_name == \"T5\":\n",
    "        inputs = [\"summarize: \" + doc for doc in examples[\"document\"]]\n",
    "    else:\n",
    "        inputs = examples[\"document\"]\n",
    "        \n",
    "    # Tokenize the input documents\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, \n",
    "        max_length=max_input_length, \n",
    "        truncation=True, \n",
    "        padding=\"longest\"\n",
    "    )\n",
    "\n",
    "    # --- Prepare Target ---\n",
    "    # Tokenize the target summaries (labels)\n",
    "    # We use the 'as_target_tokenizer' context manager for this\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples[\"summary\"], \n",
    "            max_length=max_target_length, \n",
    "            truncation=True,\n",
    "            padding=\"longest\"\n",
    "        )\n",
    "\n",
    "    # The model needs the tokenized targets to be named 'labels'\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "print(\"Preprocessing function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d44f9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dictionary to store our three new tokenized datasets\n",
    "tokenized_datasets = {}\n",
    "\n",
    "print(\"--- Applying preprocessing to all 3 model types ---\")\n",
    "\n",
    "for model_name, tokenizer in tokenizers.items():\n",
    "    print(f\"Tokenizing data for {model_name}...\")\n",
    "    \n",
    "    # We use .map() to apply our function to every example\n",
    "    # We use a 'lambda' to pass in the extra 'tokenizer' and 'model_name' arguments\n",
    "    tokenized_ds = sampled_dataset.map(\n",
    "        lambda examples: preprocess_function(examples, tokenizer, model_name),\n",
    "        batched=True  # Process examples in batches for speed\n",
    "    )\n",
    "    \n",
    "    # Store the result\n",
    "    tokenized_datasets[model_name] = tokenized_ds\n",
    "\n",
    "print(\"\\n--- Preprocessing complete! ---\")\n",
    "print(\"Tokenized datasets created:\", list(tokenized_datasets.keys()))\n",
    "print(\"\\nExample of tokenized data (BART):\")\n",
    "print(tokenized_datasets[\"BART\"]['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cea93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Saving tokenized datasets to disk ---\")\n",
    "\n",
    "for model_name, dataset in tokenized_datasets.items():\n",
    "    output_dir = f\"./tokenized_data/{model_name}\"\n",
    "    print(f\"Saving {model_name} dataset to {output_dir}\")\n",
    "    dataset.save_to_disk(output_dir)\n",
    "\n",
    "print(\"\\n--- All processed datasets saved! ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
